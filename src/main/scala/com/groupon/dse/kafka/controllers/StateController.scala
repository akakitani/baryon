/*
 * Copyright (c) 2016, Groupon, Inc.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met:
 *
 * Redistributions of source code must retain the above copyright notice,
 * this list of conditions and the following disclaimer.
 *
 * Redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution.
 *
 * Neither the name of GROUPON nor the names of its contributors may be
 * used to endorse or promote products derived from this software without
 * specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
 * IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
 * PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
 * TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

package com.groupon.dse.kafka.controllers

import com.groupon.dse.kafka.common.{State, WrappedMessage}
import com.groupon.dse.kafka.partition.Partition
import org.apache.spark.rdd.RDD

/**
  * A [[StateController]] stores the application's consumed offsets to some storage system. Implementations should be
  * serializable, as the [[StateController]] can exist on both the driver and the executors.
  *
  * Any exceptions thrown by the underlying storage system should either be caught and handled, or a
  * [[StateControllerConnectionException]] should be thrown.
  */
trait StateController extends Serializable {

  /**
    * Given a Kafka [[Partition]], generate a [[String]] that uniquely identifies this partition for this storage system
    *
    * @param seed [[Partition]] to generate a key for
    * @return a [[String]] that uniquely identifies this partition
    */
  def generateStateKey(seed: Partition): String

  /**
    * For a key generated by [[generateStateKey()]], determine if this key is already present in this storage system
    *
    * @param key [[String]] that uniquely identifies a [[Partition]]
    * @return true if [[State]] for this key exists in this storage system; false otherwise
    */
  def isKeyPresent(key: String): Boolean

  /**
    * Determine if this [[StateController]] is connected to the underlying storage system
    *
    * @return true if connected; false otherwise
    */
  def isConnected: Boolean

  /**
    * Close the connection to the underlying storage system for this [[StateController]]
    */
  def close(): Unit

  /**
    * For a key generated by [[generateStateKey()]], obtained the stored [[State]].
    *
    * If [[State]] for this key does not exist, throws a [[KeyDoesNotExistException]]
    *
    * @param key [[String]] that uniquely identifies a [[Partition]]
    * @return [[State]] for the [[Partition]] represented by the key
    */
  def getState(key: String): State

  /**
    * Set the [[State]] for a key, which should uniquely identify a [[Partition]].
    *
    * If the key already exists, [[setState()]] should overwrite the existing [[State]].
    *
    * @param key [[String]] that uniquely identifies a [[Partition]]
    * @param value new [[State]] to store
    * @return a tuple of the key and the newly set [[State]]
    */
  def setState(key: String, value: State): (String, State)

  /**
    * Set the [[State]] by using the [[WrappedMessage]] RDD.
    *
    * This method is to be used only in blocking mode.
    *
    * In blocking mode, the assumption is that every partition in an RDD uniquely corresponds to a Kafka partition. In
    * other words, the number of partitions in an RDD will always be <= the number of Kafka partitions we are consuming
    * from, and no two RDD partitions will contain data from the same Kafka partition. With this assumption, it's
    * possible to just take the first [[WrappedMessage]] of each partition and use the `batchEndOffset` field to
    * set the state for this RDD.
    *
    * @param rdd The RDD comprising of a list of [[WrappedMessage]] object, used to compute latest state
    * @return (partition key, [[State]]) tuple with the latest state
    */
  def setState(rdd: RDD[WrappedMessage]): Seq[(String, State)] = {
    val currentTimeMillis = System.currentTimeMillis()
    val partitionAndOffset = rdd.mapPartitions(partition => {
      if (partition.hasNext) {
        val wm = partition.next()
        Iterator((wm.partitionStateKey, wm.batchEndOffset))
      } else {
        Iterator.empty
      }
    }).reduceByKeyLocally((offsetA, offsetB) => math.max(offsetA, offsetB))
    val partitionAndState = partitionAndOffset.map({ case (partitionStateKey, batchEndOffset) =>
      (partitionStateKey, State(batchEndOffset, currentTimeMillis))
    })

    try {
      partitionAndState.foreach({
        case (partitionStateKey, state) => setState(partitionStateKey, state)
      })
    } catch {
      case sc: StateControllerConnectionException => throw sc
    }

    partitionAndState.toSeq
  }
}

case class KeyDoesNotExistException(message: String) extends Exception(message)

case class StateControllerConnectionException(message: String) extends Exception(message)
